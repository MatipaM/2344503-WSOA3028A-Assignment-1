<!DOCTYPE html>

<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
    <header>
        <nav> 
            <ul>
                <li><a href="index.html" class="u-url"><i class="fa-solid fa-house-chimney"> </i>  <br/> Home </a>  </li> 
            <li><a href="Development.html" class="u-url"><i class="fa-solid fa-gamepad"> </i>  <br/> Development </a>  </li> 
            <li> <a href="contactMe.html" class="u-url"><i class="fa-solid fa-address-book"></i> <br/> Contact </a></li> 
            <li> <a href="Reflections.html" class="u-url"><i class="fa-solid fa-book-open-reader"></i><br/> Reflections </a></li> 
            <li><a href="blogPosts.html" class="u-url"><i class="fa-brands fa-readme">  </i> <br/> Blogs </a> </li>
        </nav>
    </header>
<head>
    <meta name="description" content="This page contain blog posts. These posts are a critical reflection/response to the history and development of the world wide web">
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link rel="stylesheet" href="CSS/general.css">
    <link rel="stylesheet" href="CSS/CriticalWriting6.css">
    <!-- <link rel="stylesheet" href="CSS/loginPage.css"> -->
    <!-- <link rel="stylesheet" href="CSS/blogPosts.css"> -->
   
    <script src="https://kit.fontawesome.com/e9f29bfa82.js" crossorigin="anonymous"></script>
    <meta charset="utf-8" />
    <title>Matipa's Portfolio Website</title>
</head>
<section>
    <h1>
        Internet, Society and Design Justice
       
    </h1>
<p>
    In this essay I will be exploring how misrepresentation of data due to digital inequalities results in Artificial Intelligence systems which are prejudice. This prejudice arises not only against minorities but even more so against intersectional minorities. Within the context of this essay, intersectional minorities are people who are considered a minority in more than one way.
</p>

<p>
    Digital inclusion is “defined as the ability of individuals and groups to access and use information and communication technologies” (Digital Inclusion, 2022). Digital inequalities arise from a lack of digital inclusion. They can take shape in the form of internet being too expensive to access, broadband costs, or rising costs of digital devices themselves. So far, Americans have the largest accessibility to the internet, with 94.6% of their population having access, this is almost three times the accessibility of Africans, whom only 39.3% have access (Digital Inclusion, 2022).
</p>

<p>
    Lack of internet diversity means that the internet is dominated by a single culture, this is called ‘rural penalty’ (Daniolou, 2020). Lack of participation from all countries decreases diversity as well as isolates certain cultures’ world views and knowledge. This creates a system of knowledge validation, in which the perspective with the loudest influence becomes correct. Thus countries who have predominantly limited access to the internet also have predominately limited access to international knowledge production. </p>
</p>

<p>
    The paper, ‘Algorithmic Colonization of Africa’ (Birhane, 2020) highlights the key issues which result in biased AI systems. This paper explains, why robots cannot be seen as neutral. The reason being, that if the databases used to train these intelligent machines are not neutral, how can the machines be? 
</p>

<p>
    Birhane’s solution to algorithmic inequality is to centre those who are most impacted by the situation at hand. However, I would argue that this is made difficult by corporations who are mainly motivated by financial goals. Mirca Madianou in her paper, ‘Technocolonialism: Digital Innovation and Data practices in the humanitarian responses to refugee crises’ speaks about the ‘Logic of Capitalism’ (2019). In this point she expands, on how humanitarian projects do well in presenting the brand in a good light, therefore increasing visibility as well as data access. In relation to Birhane’s solution to focus on the most impacted group. The respected company may prioritise profits by collecting data which is the cheapest to collect. This may be data the company already has legal access to, e.g. their own citizens (which if predominantly one race will result in racist training data sets). Prioritisation could also be by collecting the data of vulnerable groups whose countries are willing to sell the data of their citizens. If the company intends to use the data in a way that is more profitable to the company rather than in a way which is more advantageous to the affected group, that group could be more negatively affected than intended.
</p>
    

    <p>
        Birhane also speaks about how negative biases of software tend to be seen as side effects as opposed to individual issues which are a result of deeply rooted biases embedded within the training data. These issues are ones which temporarily solutions will not be able to fix, but rather which call for an analysis of the original data. This can be seen with facial recognition software, where darker skinned females are the most likely to be misclassified, producing an error rate of 34.7% compared to lighter skin males who only have an error rate of 0.8%. These trends can be seen in the training data which tend to be mostly filled with lighter skinned individuals and/or males (Buolamwini and Gebru, 2018). 
    </p>

    <p>The large error rate seen with black females also speaks to Kimberlé Crenshaw’s 1989 article, “Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics”. Crenshaw highlights three case studies, in which black women were told that they could not contest on behalf of either black people or women. As being an intersection of these two minority groups, the court told them that they could not be seen as representatives of either group. This is an example of “[t]he single axis framework [which] erases black women in the conceptualisation, identification, and remediation of race and sex discrimination by limiting inquiry to the experience of otherwise-privileged members of the group” (Crenshaw,1989).</p>

<p>Patricia Hill Collins writes about “the matrix of domination” in her book titled, ‘Black Feminist Thought’ (1990). Collins expresses how “[e]ach individual derives varying amounts of penalty and privilege from the multiple systems of oppression which frame everyone’s lives”. With this statement, Collins explains how someone who is considered a double minority will experience more bias from systems then someone who is considered a single minority. I would also argue that how evident it is to these systems of oppression how diverse one is, also determines how one is treated. For example, a colourist AI will be better suited for lighter skinned Africans, and someone who is a single minority member of the LGBT community may not face discrimination from a facial recognition system at all.</p>
    
<p>In conclusion, intersectionality and the matrix of domination should not play a factor in how one is treated by an intelligent machine. However, it is good for data engineers (and society in general) to be aware of how disproportionate representation in data can result in biased machines.</p>
</section>

    <footer>
        <h3 class="References"> References 
        </h3>
<cite>BIRHANE, A. 2020. Algorithmic Colonization of Africa. Scripted. Vol 17. (2), August: pp: 389-411.</cite> <br/> 
<cite>BUOLAMWINI, J and GEBRU, T. 2018. Gender Shaders: Intersectional Accuracy Disparities in Commercial Gender Classification. Proceedings of Machine Learning Research. Vol 81. pp: 1-15.</cite><br/> 
<cite>COLLINS, P. 1990. Black Feminist Thought: Hyman.</cite><br/> 
<cite>CRENSHAW, K. 1989. Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics: University of Chicago.</cite><br/> 
<cite>MADIANOU, M. 2019. Technocolonialism: Digital Innovation and Data practices in the humanitarian responses to refugee crises. Social Media + Society. July-September: pp. 1-13.</cite><br/> 
<cite>2022. [online] Available at: <https://www.internetjustsociety.org/digitalinclusion> [Accessed 10 June 2022].</cite><br/> 
<cite>2022. [online] Available at: <https://www.internetjustsociety.org/the-need-for-global-internet-connectivity> [Accessed 10 June 2022].</cite><br/> 
</footer>