<!DOCTYPE html>

<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
    <header>
        <nav> 
            <!-- <input type="checkbox" id="check">
            <label for="check" class="bars"> <i class="fa-solid fa-bars"></i> </label> -->
            <ul><a href="PlantsVsInsects.html" class="u-url"><i class="fa-solid fa-gamepad"> </i>  </a> </ul>
            <ul> <a href="Firtech.html" class="u-url"><i class="fa-solid fa-tablet-screen-button"></i> </a>  </ul>
            <ul> <a href="contactMe.html" class="u-url"><i class="fa-solid fa-address-book"></i> </a></ul>
            <ul> <a href="reflections.html" class="u-url"><i class="fa-solid fa-book-open-reader"></i></a></ul>
            <ul> <a href="index.html" class="u-url"> <i class="fa-solid fa-house"></i> </a></ul>
        </nav>
    </header>
<head>
    <meta name="description" content="This page contain blog posts. These posts are a critical reflection/response to the history and development of the world wide web">
    <link rel="stylesheet" href="CSS/loginPage.css">
    <link rel="stylesheet" href="CSS/blogPosts.css">
    <link rel="stylesheet" href="CSS/general.css">
    <script src="https://kit.fontawesome.com/e9f29bfa82.js" crossorigin="anonymous"></script>
    <meta charset="utf-8" />
    <title>Matipa's Portfolio Website</title>
</head>
<!-- 
    <span class="loginIcon">
        <a href="login.html">
        <i class="fa-solid fa-user"></i>
        <br/>
        <div class="loginText p-login">
        Log in!
        </a>
    </div>
    </span> -->
<article>
    <span class="row"> 
        <span class="column">
<h1> Blog Posts</h1>
</span>
<span class="column">
<img src="Images/ComputerPixelArt.png" alt="Pixel Art of a Computer" width="15%" class="ComputerPixelArt">
</span>
</span>
</head>
<body>
    <section>
    <h2 class="heading">
Critical Writing Week 2 - Alan Turing, Computing Machinery and Intelligence Reflection
    </h2>
 
<p> Alan Turing was a Cambridge Mathematician whose paper, ‘Computing Machinery and Intelligence’ changed the way readers thought about machines and their capabilities. One of the main highlights of Turing’s paper is the Turing test. Instead of asking “Can machines think?”, Turing flips this to the imitation game. The imitation game includes three people, a man, a woman and an interrogator. It is the interrogator’s job to select which participant they think is the woman. In Turing’s version, he replaces the woman with a machine to see if the machine would be able to pass for the woman. </p>
     
<p>  Alan Turing then questions his own test by asking whether it is worthy of investigation. I found this approach to be quite interesting as it is easy to get caught up in investigating/prototyping something new for the fun of it. However, questioning the value of an investigation forces the researcher to ask themselves why they want to learn about a certain topic, and how or who it will benefit. This is also useful when coming to a conclusion with your own research as one knows what they are looking out for. </p>
     
    <p>  In the final part of Alan Turing’s paper, he discusses contrary views of the Turing Test. Turing begins with the “Theological Objection”, which is based on the view that God has given the ability to think to humans alone, therefore machines will never have the ability to think. However, I think that another side to this view would require us to question how we define thinking for a human versus thinking for a machine. With the rise of Artificial Intelligence, we have seen how it has given the computer the ability to learn and then produce based off data it has been given, this is what we have defined as thinking for a computer. Whereas thinking for a human is not necessarily a product of the input from other’s actions/data but instead a personal endeavour which requires no external input. I think this same line of thinking applies to the argument of consciousness which states that the only way to know that a computer is thinking is to be the computer thinking. </p>
     
        <p>  Another objection is that of the “head in the Sand” which sees the computer’s ability to think as dangerous. I do see this as a legitimate concern, in the same that a computer designed/trained to kill will be danger to others. However, I do not think this is necessarily only a danger of machines that can think but rather a dangerous consequence of machines which are engineered that way. The mathematical objection can prove that they are limitations to discrete-state machines however, I don’t see this as being against the creation of a computer’s ability to think but rather a temporary hurdle to be jumped over. </p>
     
            <p>  Arguments from various disabilities which imply that a computer will not have the ability to do everything are currently correct but then why would we want computers to do everything. This ignores the fact that they are things which can be done that should not be done. Turing also uses this moment to point out how often we overlook errors of functioning within machines which are mechanical/electrical faults which cause the machines to behave differently than designed. Discussions which overlook these errors are instead speaking about abstract machines. Another which also occurs frequently are errors of conclusion which only arise when we associate a specific meaning to the output of a machine. </p>
     
                <p>  An argument that the nervous system can not be mimicked by a discrete state machine as even a small error in the sizing of a nervous impulse on a neutron can make a big difference on an outgoing impulse. State machines may lack this attention to detail as they are no graduations between states. The argument of  ‘informality of behaviour’ highlights how impossible it would be to produce a set of rules for humans, let alone one so detailed that a machine could follow. This is further supported by the quote, “If each man had a definite set of rules of conduct by which he regulated his life, he would no better than a machine.”. The final argument from extra-sensory perception speaks to  “telepathy”, “clairvoyance” and “precognition” as well as  “psycho-kinesis”. These all stem from the human ability to sense another’s thoughts without verbal communication, again unspoken rules which would be a challenge to program into a computer. </p>
     
                    <p>  The final chapter of his paper, Learning Machines speaks to the future of Artificial Intelligence and how we can create an “unemotional” mode of communication in order to teach child machines (Turing, 1950). <br/><br/>

    <h3 class="references"> References 
    </h3>
<p>
<cite>TURING,A. 1950. Computing Machinery and Intelligence. <em> MIND </em>, 59,433-460</cite>
</p>

</section>
<section>
<h2 class="heading">
    Critical Writing Week 4 - What is a web browser?
</h2>
 
 
        <p>A web/internet browser collects information from the web server when a user requests to access a certain web page. The browser then displays this information on the user’s device. This information is transferred using the HTTP. HTTP stands for Hypertext Transfer Protocol. HTTP transmits hypermedia documents including HTML. A rendering engine then translates the data from the HTTP and to displays the text/images/videos on the users screen (What is a web browser?, 2022). </p>
        <p>Even though it would be ideal for browsers to render information the same way, so that the website you are viewing looks consistent regardless of which web browser you are using. Not all browsers interpret HTML in the same way. Web standards are designed for use by software engineers to encourage consistent functionality between pages on all browsers. Theses web standards are created by experts from multiple companies who come together and agree on how the technologies should work (The web and web standards - Learn web development | MDN, 2022).</p>
            <p>Each web page has its own unique URL (uniform Resource Locator) which is used to navigate to and from the page via hyperlinks. Url’s are also known as the web address, they tell the browser where to look for each item in the html and where it goes on the page. Cookies on websites save your information on your computer so that the next time you visit that same website, your preferences, login information is remembered for you (What is a URL? - Learn web development | MDN, 2022).</p>
        <h3 class="references"> References 
        </h3>

        <p>
<cite> Developer.mozilla.org. 2022. What is a URL? - Learn web development | MDN. [online] Available at: <https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_URL> [Accessed 19 April 2022]. <br/> <br/> </cite>
<cite> Mozilla. 2022. What is a web browser?. [online] Available at: <https://www.mozilla.org/en-US/firefox/browsers/what-is-a-browser/> [Accessed 19 April 2022]. <br/> <br/> </cite>
<cite> Developer.mozilla.org. 2022. The web and web standards - Learn web development | MDN. [online] Available at: <https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/The_web_and_web_standards> [Accessed 19 April 2022]. <br/><br/> </cite>
<cite> Developer.mozilla.org. 2022. HTTP | MDN. [online] Available at: <https://developer.mozilla.org/en-US/docs/Web/HTTP> [Accessed 19 April 2022]. </cite>
</p>
    </p>
</section>
<section>
    <h2 class="heading">
        Critical Writing 6 - A reflection on a brief history of Web Robots 
    </h2>
        <p>
            A reflection on, a brief history of Web Robots
</p>
 

<p>Web robots also known as web crawlers extract all the URL’s (Uniform Resource Locator) on one page (the seeds), follows them and then extracts all the links on the new page. The aim of this process is to learn what is on every site so that when a user searches for information the most appropriate result will be suggested. The links need to be organised in terms of relativity to the user’s search content. Web crawlers are operated by search engines and are search indexers. Search indexing involves the act of marking webpages so they can be found easily later. It is dependent mostly on text but also metadata. Metadata is data that describes other data. In the case of webpages, metadata communicates what each web page is about. Data in the meta title/description is prioritised over text within the page when web crawlers index and organise web pages. Web crawlers use the number of visitors a site gets, the number of other webpages that link to that webpage, as well as other information on the webpage to decide which one to crawl first. This is so the web crawler can prioritise web pages which appear to be most relevant to that piece of information. URL Normalisation is used to try and avoid web crawlers searching the same URL more than once unintentionally (What Is a Web Crawler? | How Web Spiders Work | Cloudflare UK, n.d.). </p>

 

 

<p>The websites and order in which a web crawler will index are dependent on certain policies. These policies influence how often the web crawler checks for updates of the pages’ content. Web crawlers are required to follow a ‘selection policy’. A selection policy specifies which pages the web crawler should download. They also have a ‘re-visit policy’ which tells the crawler when to check for content updates on the webpage. A ‘parallelization policy’ which contains information on how web crawlers should be organised and distributed amongst web pages. A ‘politeness policy’ which guides the crawlers on how to not overload the website when indexing/storing data. This is crucial as crawlers are interacting with so much data, that they have the ability to overwhelm the server. Therefore the politeness policy as well as scheduling, aids in the prevention of web crawlers overloading of a server. (Wikipedia Contributors, 2019) </p>

 

 

<p>Web crawlers are required to make requests to the server before they can crawl that website. A webpage can refuse this service by adding the tag “no index” to their landing page. This will mean the site will not show up in the search engine. The tag, “disallow” can also be used in the robots.txt file and the website will not be crawled. Developers may decide to not want their website crawled in cases where only certain people with a link can access their website or to avoid the negative effects of performance, web crawlers can cause the site to have (Wikipedia Contributors, 2019). The robots.txt file contains a list of protocols the crawler must follow when searching/indexing the website. </p>

 

<h3 class="references"> References 
</h3>

<p>
<cite> What Is a Web Crawler? | How Web Spiders Work | Cloudflare UK. (n.d.). Cloudflare. [online] Available at: https://www.cloudflare.com/en-gb/learning/bots/what-is-a-web-crawler/. </cite>

 <br/><br/>

<cite> Wikipedia Contributors (2019). Web crawler. [online] Wikipedia. Available at: https://en.wikipedia.org/wiki/Web_crawler. </cite>
     
        </p>
    </section>
    </article>
</body>
<footer>
    <cite>
        LLC, C., 2022. LilBitOfSage on Planet Minecraft. [online] Planet Minecraft. Available at: <https://www.planetminecraft.com/member/lilbitofsage/> [Accessed 22 April 2022].    </cite>
</footer>
</html>
